
Neovim Inline LLM Plugin Specification1. ObjectiveTo create a lightweight Neovim plugin in Lua that enables a user to get an inline, contextual response from a local Large Language Model (LLM) within a Markdown file.2. Core Feature: Inline QueryThe primary function is to send a user's selected text to an LLM and insert the response directly into the buffer.User Workflow:The user opens a Markdown file.The user enters Visual Mode (v, V, or CTRL-V) and selects a block of text.The user triggers the plugin via a command or keymap (e.g., <leader>ll).The selected text is sent as a prompt to the configured LLM API.The LLM's response is streamed back and inserted directly below the visual selection.The response must be formatted as a Markdown blockquote (i.e., each line prefixed with > ).3. Technical RequirementsLanguage: The plugin must be written entirely in Lua.Backend Service: The plugin will interface with a locally running Ollama instance.API Endpoint: It will use the /api/generate endpoint of the Ollama API.Communication:All API calls must be asynchronous to prevent locking or freezing the Neovim UI.Use vim.fn.jobstart() or an equivalent non-blocking method with curl to perform the HTTP POST request.Dependencies: The plugin's core functionality will depend on curl being available on the user's system.Error Handling: Provide user feedback via vim.notify() for API connection errors or if curl is not found.4. User Interface & ConfigurationCommand: The plugin must expose a user command for this functionality, e.g., :LLMQuery. This command should operate on a visual range.Keymap: A default keymap for Visual Mode must be provided, e.g., <leader>ll. The user should be able to override this.Configuration: The user must be able to configure the following in their init.lua via a setup() function:model: The name of the Ollama model to use (e.g., "mistral").url: The base URL for the Ollama API (e.g., "http://localhost:11434").Feedback:Use vim.notify() to display the status (e.g., "Querying LLM...").Use vim.notify() to display the result (e.g., "Response received.").5. Scope: ExclusionsThis initial version will not include a chat window or management of conversation history.It will not support LLM APIs other than Ollama.Text modification (e.g., refactoring selected code) is out of scope. The sole function is to insert a response.Streaming the response word-by-word is a future enhancement, not a requirement for v1. The initial implementation can insert the full response upon completion.
